{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Maximum Collector Current Voltage from Digikey Datasheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook will begin extracting additional relations from transistors (`eb_v_max`, `c_current_max`, `dev_dissipation`, `dc_gain_min`).\n",
    "\n",
    "Sarting with the `max_emitter_base_voltage` as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KBC Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a new database named `eb_v_max` for extracting the maximum ratings for emitter - base voltage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Configure logging for Fonduer\n",
    "logging.basicConfig(stream=sys.stdout, format='[%(levelname)s] %(name)s - %(message)s')\n",
    "log = logging.getLogger('fonduer')\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "PARALLEL = 4 # assuming a quad-core machine\n",
    "ATTRIBUTE = \"eb_v_max\"\n",
    "conn_string = 'postgresql://localhost:5432/' + ATTRIBUTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Parsing and Transforming the Input Documents into Unified Data Models\n",
    "\n",
    "We first initialize a `Meta` object, which manages the connection to the database automatically, and enables us to save intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fonduer.meta - Connecting user:None to localhost:5432/eb_v_max\n",
      "[INFO] fonduer.meta - Initializing the storage schema\n"
     ]
    }
   ],
   "source": [
    "from fonduer import Meta\n",
    "\n",
    "session = Meta.init(conn_string).Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring an `HTMLDocPreprocessor`\n",
    "We start by setting the paths to where our documents are stored, and defining a `HTMLDocPreprocessor` to read in the documents found in the specified paths. `max_docs` specified the number of documents to parse.\n",
    "\n",
    "123 HTML files in `/dev/html`, 76 HTML files in `/test/html`, 2745 HTML files in `/train_digikey/html`.\n",
    "\n",
    "This test run is parsing files from `/test/html`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.parser.preprocessors import HTMLDocPreprocessor\n",
    "from fonduer.parser import Parser\n",
    "\n",
    "docs_path = 'data/test/html/'\n",
    "pdf_path = 'data/test/pdf/'\n",
    "\n",
    "max_docs = 100\n",
    "doc_preprocessor = HTMLDocPreprocessor(docs_path, max_docs=max_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring a `Parser`\n",
    "Next, we configure a `Parser`, which serves as our `CorpusParser` for PDF documents. We use [spaCy](https://spacy.io/) as a preprocessing tool to split our documents into sentences and tokens, and to provide annotations such as part-of-speech tags and dependency parse structures for these sentences. In addition, we can specify which modality information to include in the unified data model for each document. Below, we enable all modality information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fonduer.utils.udf - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcf969be24f48728de01d9316e7dff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=75), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.5 s, sys: 427 ms, total: 6.93 s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = Parser(session, structural=True, lingual=True, visual=True, pdf_path=pdf_path)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking to ensure consistency in document numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 75\n",
      "Sentences: 37683\n"
     ]
    }
   ],
   "source": [
    "from fonduer.parser.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Mention Extraction, Candidate Extraction Multimodal Featurization\n",
    "\n",
    "Given the unified data model from Phase 1, `Fonduer` extracts relation\n",
    "candidates based on user-provided **matchers** and **throttlers**. Then,\n",
    "`Fonduer` leverages the multimodality information captured in the unified data\n",
    "model to provide multimodal features for each candidate.\n",
    "\n",
    "## 2.1 Mention Extraction\n",
    "\n",
    "We first start by defining and naming our two `mention`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models import mention_subclass\n",
    "\n",
    "Part = mention_subclass(\"Part\")\n",
    "EB_Voltage = mention_subclass(\"EB_Voltage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transistor Part Number Matchers\n",
    "\n",
    "Previously defined transistor part number matchers as found in the `maximum_storage_tempature.ipynb` tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.matchers import RegexMatchSpan, DictionaryMatch, LambdaFunctionMatcher, Intersect, Union\n",
    "\n",
    "### Transistor Naming Conventions as Regular Expressions ###\n",
    "eeca_rgx = r'([ABC][A-Z][WXYZ]?[0-9]{3,5}(?:[A-Z]){0,5}[0-9]?[A-Z]?(?:-[A-Z0-9]{1,7})?(?:[-][A-Z0-9]{1,2})?(?:\\/DG)?)'\n",
    "jedec_rgx = r'(2N\\d{3,4}[A-Z]{0,5}[0-9]?[A-Z]?)'\n",
    "jis_rgx = r'(2S[ABCDEFGHJKMQRSTVZ]{1}[\\d]{2,4})'\n",
    "others_rgx = r'((?:NSVBC|SMBT|MJ|MJE|MPS|MRF|RCA|TIP|ZTX|ZT|ZXT|TIS|TIPL|DTC|MMBT|SMMBT|PZT|FZT|STD|BUV|PBSS|KSC|CXT|FCX|CMPT){1}[\\d]{2,4}[A-Z]{0,5}(?:-[A-Z0-9]{0,6})?(?:[-][A-Z0-9]{0,1})?)'\n",
    "\n",
    "part_rgx = '|'.join([eeca_rgx, jedec_rgx, jis_rgx, others_rgx])\n",
    "part_rgx_matcher = RegexMatchSpan(rgx=part_rgx, longest_match_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a matcher from a dictionary of known part numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def get_digikey_parts_set(path):\n",
    "    \"\"\"\n",
    "    Reads in the digikey part dictionary and yeilds each part.\n",
    "    \"\"\"\n",
    "    all_parts = set()\n",
    "    with open(path, \"r\") as csvinput:\n",
    "        reader = csv.reader(csvinput)\n",
    "        for line in reader:\n",
    "            (part, url) = line\n",
    "            all_parts.add(part)\n",
    "    return all_parts\n",
    "\n",
    "### Dictionary of known transistor parts ###\n",
    "dict_path = 'data/digikey_part_dictionary.csv'\n",
    "part_dict_matcher = DictionaryMatch(d=get_digikey_parts_set(dict_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use user-defined functions to further improve our matchers. For example, here we use patterns in the document filenames as a signal for whether a span of text in a document is a valid transistor part number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import range\n",
    "\n",
    "def common_prefix_length_diff(str1, str2):\n",
    "    for i in range(min(len(str1), len(str2))):\n",
    "        if str1[i] != str2[i]:\n",
    "            return min(len(str1), len(str2)) - i\n",
    "    return 0\n",
    "\n",
    "def part_file_name_conditions(attr):\n",
    "    file_name = attr.sentence.document.name\n",
    "    if len(file_name.split('_')) != 2: return False\n",
    "    if attr.get_span()[0] == '-': return False\n",
    "    name = attr.get_span().replace('-', '')\n",
    "    return any(char.isdigit() for char in name) and any(char.isalpha() for char in name) and common_prefix_length_diff(file_name.split('_')[1], name) <= 2\n",
    "\n",
    "add_rgx = '^[A-Z0-9\\-]{5,15}$'\n",
    "\n",
    "part_file_name_lambda_matcher = LambdaFunctionMatcher(func=part_file_name_conditions)\n",
    "part_file_name_matcher = Intersect(RegexMatchSpan(rgx=add_rgx, longest_match_only=True), part_file_name_lambda_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can union all of these matchers together to form our final part matcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_matcher = Union(part_rgx_matcher, part_dict_matcher, part_file_name_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emitter - Base Voltage Matchers\n",
    "\n",
    "Our emitter base voltage matcher can be a very simple regular expression\n",
    "since we know that we are looking for floats (e.g. 4.0, 5.0, 6.0), and by inspecting a portion of\n",
    "our corpus, we see that emitter base voltages fall within a fairly\n",
    "narrow range. (i.e. `4.0`,`5.0`,`6.0`,`7.0`, up to `10`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: This is super specific. Came from previously defined snorkel matchers at https://github.com/fonduer-apps/snorkel/blob/semi-structured/tutorials/tables/deprecated/eb_v_max.ipynb\n",
    "eb_voltage_matcher = RegexMatchSpan(rgx=r'\\-?([56]|12)(\\.0)?', longest_match_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `MentionSpace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hardware_spaces import MentionNgramsPart, MentionNgrams\n",
    "    \n",
    "part_ngrams = MentionNgramsPart(parts_by_doc=None, n_max=3)\n",
    "eb_voltage_ngrams = MentionNgrams(n_max=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Mention Extraction \n",
    "\n",
    "Next, we create a `MentionExtractor` to extract the mentions from all of\n",
    "our documents based on the `MentionSpace` and matchers we defined above.\n",
    "\n",
    "View the API for the MentionExtractor on [ReadTheDocs](https://fonduer.readthedocs.io/en/latest/user/candidates.html#fonduer.candidates.MentionExtractor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates import MentionExtractor \n",
    "\n",
    "mention_extractor = MentionExtractor(\n",
    "    session, [Part, EB_Voltage], [part_ngrams, eb_voltage_ngrams], [part_matcher, eb_voltage_matcher]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we run the extractor on all of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fonduer.candidates.mentions - Clearing table: part\n",
      "[INFO] fonduer.candidates.mentions - Clearing table: eb__voltage\n",
      "[INFO] fonduer.utils.udf - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4785608cc7d24fb9bdab54c03f320254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=75), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Mentions: 1827\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "from fonduer.candidates.models import Mention\n",
    "\n",
    "mention_extractor.apply(docs, parallelism=PARALLEL)\n",
    "\n",
    "print(\"Total Mentions: {}\".format(session.query(Mention).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Candidate Extraction\n",
    "\n",
    "Now that we have both defined and extracted the Mentions that can be used to compose Candidates, we are ready to move on to extracting Candidates. Like we did with the Mentions, we first define what each candidate schema looks like. In this example, we create a candidate that is composed of a `Part` and a `Temp` mention as we defined above. We name this candidate \"PartEBVoltage\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models import candidate_subclass\n",
    "\n",
    "PartEBVoltage = candidate_subclass(\"PartEBVoltage\", [Part, EB_Voltage])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining candidate `Throttlers`\n",
    "\n",
    "Here, we create a throttler that discards candidates if they are in the same table, but the part and max emitter base voltage are not vertically or horizontally aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.utils.data_model_utils import *\n",
    "import re\n",
    "\n",
    "def stg_eb_voltage_filter(c):\n",
    "    (part, attr) = c\n",
    "    if same_table((part, attr)):\n",
    "        return (is_horz_aligned((part, attr)) or is_vert_aligned((part, attr)))\n",
    "    return True\n",
    "\n",
    "eb_voltage_throttler = stg_eb_voltage_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the `CandidateExtractor`\n",
    "\n",
    "Now, we have all the component necessary to perform candidate extraction. We have defined the Mentions that compose each candidate and a throttler to prunes away excess candidates. We now can define the `CandidateExtractor` with the candidate subclass and corresponding throttler to use.\n",
    "\n",
    "View the API for the CandidateExtractor on [ReadTheDocs](https://fonduer.readthedocs.io/en/docstrings/user/candidates.html#fonduer.candidates.CandidateExtractor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates import CandidateExtractor\n",
    "\n",
    "\n",
    "candidate_extractor = CandidateExtractor(session, [PartEBVoltage], throttlers=[eb_voltage_throttler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fonduer.candidates.candidates - Clearing table part_eb_voltage (split 0)\n",
      "[INFO] fonduer.utils.udf - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aeb497366294f75af809568384ab3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=75), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "candidate_extractor.apply(docs,parallelism=PARALLEL,progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Candidates: 17463\n"
     ]
    }
   ],
   "source": [
    "from fonduer.candidates.models import Candidate\n",
    "\n",
    "print(\"Total Candidates: {}\".format(session.query(Candidate).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multimodal Featurization\n",
    "Unlike dealing with plain unstructured text, `Fonduer` deals with richly formatted data, and consequently featurizes each candidate with a baseline library of multimodal features. \n",
    "\n",
    "### Featurize with `Fonduer`'s optimized Postgres Featurizer\n",
    "We now annotate the candidates in our training, dev, and test sets with features. The `Featurizer` provided by `Fonduer` allows this to be done in parallel to improve performance.\n",
    "\n",
    "View the API provided by the `Featurizer` on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/features.html#fonduer.features.Featurizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fonduer.features.featurizer - Clearing Features (split 0)\n",
      "[INFO] fonduer.utils.udf - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6254cb23c8b84a52975d0c010d966d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=75), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from fonduer.features import Featurizer\n",
    "\n",
    "featurizer = Featurizer(session, [PartEBVoltage])\n",
    "featurizer.apply(split=0, train=True, parallelism=PARALLEL, progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this phase, `Fonduer` has generated the set of candidates and the feature matrix. Note that Phase 1 and 2 are relatively static and typically are only executed once during the KBC process.\n",
    "\n",
    "# Phase 3: Probabilistic Relation Classification\n",
    "In this phase, `Fonduer` applies user-defined **labeling functions**, which express various heuristics, patterns, and [weak supervision](http://hazyresearch.github.io/snorkel/blog/weak_supervision.html) strategies to label our data, to each of the candidates to create a label matrix that is used by our data programming engine.\n",
    "\n",
    "In the wild, hand-labeled training data is rare and expensive. A common scenario is to have access to tons of unlabeled training data, and have some idea of how to label them programmatically. For example:\n",
    "* We may be able to think of text patterns that would indicate a part and polarity mention are related, for example the word \"temperature\" appearing between them.\n",
    "* We may have access to an external knowledge base that lists some pairs of parts and polarities, and can use these to noisily label some of our mention pairs.\n",
    "Our labeling functions will capture these types of strategies. We know that these labeling functions will not be perfect, and some may be quite low-quality, so we will model their accuracies with a generative model, which `Fonduer` will help us easily apply.\n",
    "\n",
    "Using data programming, we can then train machine learning models to learn which features are the most important in classifying candidates.\n",
    "\n",
    "### Loading Gold Data\n",
    "TODO: Annotate datasheets for `PartEBVoltage`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Functions\n",
    "\n",
    "The emitter base voltage symbol (found in the same row as `EB_Voltage`) is usually the following: <sup>V</sup>EBO\n",
    "\n",
    "The `EB_Voltage` is also often found in the same row as a V (indicating voltage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def set_all_in_set(a, b):\n",
    "    '''return true if all of a is in b'''\n",
    "    return b.issuperset(a)\n",
    "\n",
    "def set_none_in_set(a, b):\n",
    "    '''return true if none of a is in b'''\n",
    "    return (b.difference(a) == b)\n",
    "\n",
    "def set_any_in_set(a, b):\n",
    "    '''return true if any of a is in b'''\n",
    "    return len(b.intersection(a)) > 0\n",
    "\n",
    "LFs = []\n",
    "\n",
    "###################################################################\n",
    "# POSITIVE\n",
    "###################################################################\n",
    "\n",
    "def LF_voltage_inside_table(c):\n",
    "    return 1 if c.voltage.parent.row is not None else 0\n",
    "LFs.append(LF_voltage_inside_table)\n",
    "\n",
    "# def LF_part_is_aligned(c):\n",
    "#     return 1 if (c.part.parent.table == c.voltage.parent.table and\n",
    "#                 (c.part.parent.row_num == c.voltage.parent.row_num or\n",
    "#                  c.part.parent.col_num == c.voltage.parent.col_num)) else 0\n",
    "# LFs.append(LF_part_is_aligned)\n",
    "    \n",
    "def LF_ce_keywords(c):\n",
    "    individuals = set(['collector', 'emitter', 'voltage'])\n",
    "    together = set(['collector-emitter', 'voltage'])\n",
    "    row_ngrams = set(x.replace(' ', '') for x in get_row_ngrams(c.voltage, infer=True))\n",
    "    if set_all_in_set(individuals, row_ngrams):\n",
    "        return 1\n",
    "    if set_all_in_set(together, row_ngrams):\n",
    "        return 1\n",
    "    return 0\n",
    "LFs.append(LF_ce_keywords)\n",
    "\n",
    "def LF_pos_keywords_in_row(c):\n",
    "    pos_keys = set(['v ceo', 'ceo', 'vceo', 'value', 'rating'])\n",
    "    ngrams = set(get_row_ngrams(c.voltage, infer=True))\n",
    "    if set_any_in_set(pos_keys, ngrams):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "LFs.append(LF_pos_keywords_in_row)\n",
    "\n",
    "def LF_low_table_num(c):\n",
    "    if c.voltage.parent.table <= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "LFs.append(LF_low_table_num)\n",
    "\n",
    "def LF_whole_phrase_in_row(c):\n",
    "    row_ngrams = set(get_row_ngrams(c.voltage))\n",
    "    if 'collector-emitter voltage' in row_ngrams:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "LFs.append(LF_whole_phrase_in_row)\n",
    "\n",
    "\n",
    "###################################################################\n",
    "# NEGATIVE\n",
    "###################################################################\n",
    "\n",
    "def LF_specific_neg_row_keywords(c):\n",
    "    left_ngrams = set(get_row_ngrams(c.voltage, infer=True))\n",
    "    neg_keys = set(['continuous', 'dc', 'cut-off'])\n",
    "    if set_any_in_set(neg_keys, left_ngrams):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "LFs.append(LF_specific_neg_row_keywords)\n",
    "\n",
    "def LF_equals_in_row(c):\n",
    "    row_ngrams = set(get_row_ngrams(c.voltage))\n",
    "    if '=' in row_ngrams:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "LFs.append(LF_equals_in_row)\n",
    "\n",
    "def LF_i_in_row(c):\n",
    "    row_ngrams = set(get_row_ngrams(c.voltage))\n",
    "    if 'i' in row_ngrams:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "LFs.append(LF_i_in_row)\n",
    "\n",
    "def LF_first_row(c):\n",
    "    if c.voltage.parent.row_num == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "LFs.append(LF_first_row)\n",
    "    \n",
    "def LF_not_ce_relevant(c):\n",
    "    ce_keywords = set(['collector', 'emitter', 'collector-emitter'])\n",
    "    ngrams = set(get_aligned_ngrams(c.voltage))\n",
    "    if not set_any_in_set(ce_keywords, ngrams):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "LFs.append(LF_not_ce_relevant)\n",
    "\n",
    "def LF_too_many_numbers_row(c):\n",
    "    num_numbers = list(get_row_ngrams(c.voltage, attrib=\"ner_tags\")).count('number')\n",
    "    return -1 if num_numbers >= 4 else 0\n",
    "LFs.append(LF_too_many_numbers_row)\n",
    "\n",
    "def LF_negative_keywords(c):\n",
    "    row_neg_keys = set(['ambient',\n",
    "                    'small-signal',\n",
    "                    'cut-off',\n",
    "                    'na',\n",
    "                    'ma',\n",
    "                    'cex',\n",
    "                    'resistance',\n",
    "                    'power',\n",
    "                    'junction',\n",
    "                    'dissipation', \n",
    "                    'breakdown',\n",
    "                    'current',\n",
    "                    'cbo',\n",
    "                    'vcbo'\n",
    "                    'peak',\n",
    "                    '=',\n",
    "                    'f',\n",
    "                    'p',\n",
    "                    'base',\n",
    "                    'mw',\n",
    "                    'ebo',\n",
    "                    'vebo',\n",
    "                    'i c',\n",
    "                    'total',\n",
    "                    'device',\n",
    "                    'c',\n",
    "                    'mhz',\n",
    "                    'temperature',\n",
    "                    'saturation',\n",
    "                    'operating',\n",
    "                    'storage'\n",
    "                    'bandwidth',\n",
    "                    'derate',\n",
    "                    'above',\n",
    "                    'product',\n",
    "                    'figure',\n",
    "                    'conditions',\n",
    "                    'current gain',\n",
    "                    'saturation',\n",
    "                    'min',\n",
    "                    'min.',\n",
    "                    'typ',\n",
    "                    'typ.',\n",
    "                    'max',\n",
    "                    'max.',\n",
    "                    'gain',\n",
    "                    'p',\n",
    "                    'thermal',\n",
    "                    'test'])\n",
    "    row_ngrams = set(get_row_ngrams(c.voltage))\n",
    "    col_ngrams = set(get_col_ngrams(c.voltage))\n",
    "    col_neg_keys = set(['conditions', \n",
    "                        'condition', \n",
    "                        'parameter', \n",
    "                        'min',\n",
    "                        'min.',\n",
    "                        'typ',\n",
    "                        'typ.',\n",
    "                        'max',\n",
    "                        'max.',\n",
    "                        'test'])\n",
    "    if set_any_in_set(row_neg_keys, row_ngrams):\n",
    "        return -1\n",
    "    if set_any_in_set(col_neg_keys, col_ngrams):\n",
    "        return -1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "LFs.append(LF_negative_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the Labeling Functions\n",
    "\n",
    "Next, we need to actually run the LFs over all of our training candidates, producing a set of `Labels` and `LabelKeys` (just the names of the LFs) in the database. Note that this will delete any existing `Labels` and `LabelKeys` for this candidate set.\n",
    "\n",
    "View the API provided by the `Labeler` on [ReadTheDocs](https://fonduer.readthedocs.io/en/stable/user/supervision.html#fonduer.supervision.Labeler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.supervision import Labeler\n",
    "\n",
    "labeler = Labeler(session, [PartEBVoltage])\n",
    "%time labeler.apply(split=0, train=True, lfs=[LFs], parallelism=PARALLEL, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
